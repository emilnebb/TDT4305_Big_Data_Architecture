{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prosjekt del 1 - Emil Neby + Cornelia Plesner",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilnebb/TDT4305_Big_Data_Architecture/blob/main/Prosjekt_del_1_Emil_Neby_%2B_Cornelia_Plesner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB16uBsJDrxY"
      },
      "source": [
        "### I used contents from these sources to create this Colab notebook: \n",
        "  1. https://colab.research.google.com/github/asifahmed90/pyspark-ML-in-Colab/blob/master/PySpark_Regression_Analysis.ipynb\n",
        "  2. https://gist.github.com/dvainrub/b6178dc0e976e56abe9caa9b72f73d4a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LupFC2QBLB9P"
      },
      "source": [
        "# **OUTCOME: having an enviornment to develop Spark apps in Python3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "## **Step 0: setting things up in Google Colab**\n",
        "\n",
        "First, we need to install all the dependencies in Colab environment like Apache `Spark 3 with Hadoop 2.7`, `Python 3.6`, `Java 11` (and a helper Python package named `Findspark`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!rm -rf spark-3.0.2-bin-hadoop2.7.tgz*\n",
        "!pip -q install findspark pyspark"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set some environment variables. We need to set the values for `JAVA_HOME` and `SPARK_HOME` (and `HADOOP_HOME`), as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\"\n",
        "os.environ[\"HADOOP_HOME\"] = os.environ[\"SPARK_HOME\"]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mjULZnTHVnU"
      },
      "source": [
        "## **Step 1: downloading project's dataset**\n",
        "Now let's download the project's dataset from Github. You can read the dataset for the course's project from `datasets/data/TDT4305_S2021`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG6eGOe7Hcu_",
        "outputId": "eacc735e-d147-460b-ec67-1e1ff225651b"
      },
      "source": [
        "!rm -rf datasets\n",
        "!git clone --depth=1 -q https://github.com/habedi/datasets\n",
        "!ls datasets/data/TDT4305_S2021"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " badges.csv.gz\t  'Description of the data.pdf'   users.csv.gz\n",
            " comments.csv.gz   posts.csv.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "## **Step 2: checking the Spark installation**\n",
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48lL1JV3M87K"
      },
      "source": [
        "## **Step 3: making a helper method for creating a SaprkContext variable**\n",
        "You can use `init_spark` to create a new `SaprkContext variable` and use it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKP2o0UyIvFZ"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def init_spark(app_name=\"HelloWorldApp\", execution_mode=\"local[*]\"):\n",
        "  spark = SparkSession.builder.master(execution_mode).appName(app_name).getOrCreate()\n",
        "  sc = spark.sparkContext\n",
        "  return spark, sc"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2HGl1xmN7pI"
      },
      "source": [
        "## **Step 4: a HelloWorld Spark app**\n",
        "\n",
        "Our first Spark application; it takes a list of numbers and squares each element and returns the list of squared numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnR5C9jhBHsB",
        "outputId": "c9f3d9b3-bfb9-49ba-d1f3-09afe22847fe"
      },
      "source": [
        "def main1():\n",
        "  _, sc = init_spark()\n",
        "  nums = sc.parallelize([1, 2, 3, 4])\n",
        "  print(nums.map(lambda x: x*x).collect())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main1()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 4, 9, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHA_COdYOKnO"
      },
      "source": [
        "## **Step 5: another Saprk app that loades a CSV files into an RDD**\n",
        "Apps that loads the `users.csv.gz`, into an RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DySSc6Kj1scJ"
      },
      "source": [
        "# Spark Project Part 1\n",
        "The goal of part 1 of this project is to become familiar with Big Data processing, Spark and useSpark to carry out a simple analysis of data provided.Big Data is characterized as large and/or complex datasets, that requires specialized applications toprocess its content.  By providing large amounts of valueable information, it may help stakeholdersrecognize or obtain an understanding of various insights and thus be a large part of decision-making.[2]To operate on Big Data, a framework commonly used is Apache Spark.  It is supposed to efficientlyrun applications while supporting several languages and advanced analytics[1].  The fundamentaldata structure in Apache Spark is the Resilient Distributed Datasets (RDD)[3].  The RDDs areimmutable,  partitioned collections of objects,  where different actions can be executed to collectvarious results.  A DataFrame (DF) is a distributed collection of rad-objects, and is equivalent toa table in a relation database system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3jnIXx0OOI2"
      },
      "source": [
        "#Task 1\n",
        "\n",
        "The first task consists of five subtasks, mainly regarding loading the different csv-files into RDDs.We solved this with the textFile()-command for each of the files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtgDYgGNyR83"
      },
      "source": [
        "### Task 1.1\n",
        "**Load the posts.csv.gz into an RDD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-423i_81J7d",
        "outputId": "97700563-888b-45a4-cfec-15052f84be2f"
      },
      "source": [
        "def loadPosts():\n",
        "  _, sc = init_spark()\n",
        "  rddP = sc.textFile('datasets/data/TDT4305_S2021/posts.csv.gz')\n",
        "  print(\"The 'posts.csv.gz' file is loaded into the RDD 'rddP'\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  loadPosts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 'posts.csv.gz' file is loaded into the RDD 'rddP'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbZoqIMB1MyW"
      },
      "source": [
        "###Task 1.2\n",
        "**Load the comments.csv.gz into an RDD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-7AEh3L1Q7B",
        "outputId": "558c5e5c-e774-459c-e84b-ebdd1ce78fef"
      },
      "source": [
        "def loadComments():\n",
        "  _, sc = init_spark()\n",
        "  rddC = sc.textFile('datasets/data/TDT4305_S2021/comments.csv.gz')\n",
        "  print(\"The 'comments.csv.gz' file is loaded into the RDD 'rddC'\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  loadComments()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 'comments.csv.gz' file is loaded into the RDD 'rddC'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdVn4UEF1TdF"
      },
      "source": [
        "###Task 1.3\n",
        "**Load the users.csv.gz into an RDD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ_HJHv51XW-",
        "outputId": "7d83a62d-a8e5-4c38-edbb-2b2aae50e245"
      },
      "source": [
        "def loadUsers():\n",
        "  _, sc = init_spark()\n",
        "  rddU = sc.textFile('datasets/data/TDT4305_S2021/users.csv.gz')\n",
        "  print(\"The 'users.csv.gz' file is loaded into the RDD 'rddU'\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  loadUsers()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 'users.csv.gz' file is loaded into the RDD 'rddU'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV97kjq71ZyH"
      },
      "source": [
        "###Task 1.4\n",
        "**Load the badges.csv.gz into an RDD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpZ7N3Ex1dkF",
        "outputId": "425a0441-75b1-4738-efaa-61e62d4f5b3f"
      },
      "source": [
        "def loadBadges():\n",
        "  _, sc = init_spark()\n",
        "  rddB = sc.textFile('datasets/data/TDT4305_S2021/badges.csv.gz')\n",
        "  print(\"The 'badges.csv.gz' file is loaded into the RDD 'rddB'\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  loadBadges()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 'badges.csv.gz' file is loaded into the RDD 'rddB'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZw17g_S1f_g"
      },
      "source": [
        "###Task 1.5\n",
        "**Print the number of rows for each of four RDDs.**\n",
        "To count the number of rows in each RDD, thecount()-command was used before printing.  Forthis to work, it was important to ensure that the string in the file was separated by the delimitercharacter('\\t'), so that there were produced multiple columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsVzpGIt1i_o",
        "outputId": "70b89105-d91c-40ea-9605-a3ae97d27af3"
      },
      "source": [
        "_, sc = init_spark()\n",
        "\n",
        "\n",
        "rddPosts = sc.textFile('datasets/data/TDT4305_S2021/posts.csv.gz')\n",
        "rddP = rddPosts.map(lambda x: x.split('\\t'))\n",
        "#print(\"The 'posts.csv.gz' file is loaded into the RDD 'rddP'\")\n",
        "\n",
        "rddComments = sc.textFile('datasets/data/TDT4305_S2021/comments.csv.gz')\n",
        "rddC = rddComments.map(lambda x: x.split('\\t'))\n",
        "#print(\"The 'comments.csv.gz' file is loaded into the RDD 'rddC'\")\n",
        "\n",
        "rddUsers = sc.textFile('datasets/data/TDT4305_S2021/users.csv.gz')\n",
        "rddU = rddUsers.map(lambda x: x.split('\\t'))\n",
        "#print(\"The 'users.csv.gz' file is loaded into the RDD 'rddU'\")\n",
        "\n",
        "rddBadges = sc.textFile('datasets/data/TDT4305_S2021/badges.csv.gz')\n",
        "rddB = rddBadges.map(lambda x: x.split('\\t'))\n",
        "#print(\"The 'badges.csv.gz' file is loaded into the RDD 'rddB'\")\n",
        "\n",
        "def printRows():\n",
        "  numberOfRowsP = rddP.count()\n",
        "  numberOfRowsC = rddC.count()\n",
        "  numberOfRowsU = rddU.count()\n",
        "  numberOfRowsB = rddB.count()\n",
        "  print(\"Count of rows in Posts: \"+ str(numberOfRowsP))\n",
        "  print(\"Count of rows in Comments: \"+ str(numberOfRowsC))\n",
        "  print(\"Count of rows in Users: \"+ str(numberOfRowsU))\n",
        "  print(\"Count of rows in Badges: \"+ str(numberOfRowsB))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  printRows()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of rows in Posts: 56218\n",
            "Count of rows in Comments: 58736\n",
            "Count of rows in Users: 91617\n",
            "Count of rows in Badges: 105641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwxD_EcfBz_l",
        "outputId": "8da65ed8-2ab6-401d-e752-5a6afce8dfd7"
      },
      "source": [
        "_, sc = init_spark()\n",
        "posts = sc.textFile('datasets/data/TDT4305_S2021/posts.csv.gz').map(lambda x: x.split('\\tab'))\n",
        "users = sc.textFile('datasets/data/TDT4305_S2021/users.csv.gz').map(lambda x: x.split('\\tab'))\n",
        "badges = sc.textFile('datasets/data/TDT4305_S2021/badges.csv.gz').map(lambda x: x.split('\\tab'))\n",
        "comments = sc.textFile('datasets/data/TDT4305_S2021/comments.csv.gz').map(lambda x: x.split('\\tab'))\n",
        "\n",
        "postscolumns = ['Id', 'PostTypeId', 'CreationDate','Score','ViewCount',\"Body\",'OwnerUserId','LastActivityDate',\"Title\",\"Tags\",'AnswerCount','CommentCount','FavoriteCount','Closedate']\n",
        "commentscolumns = ['PostId', \"Score\", \"Text\", \"CreationDate\", 'UserId']\n",
        "badgescolumns = ['UserId', \"Name\", \"Date\", \"Class\"]\n",
        "userscolumns = ['Id', \"Reputation\", \"CreationDate\", \"DisplayName\", \"LastAccessDate\", \"AboutMe\", \"Views\", \"UpVotes\", \"DownVotes\"]\n",
        "\n",
        "rowsPosts = posts.count()\n",
        "print(\"Rows in Posts:\" + str(rowsPosts))\n",
        "\n",
        "rowsUsers = users.count()\n",
        "print(\"Rows in Users:\" + str(rowsUsers))\n",
        "\n",
        "rowsComments = comments.count()\n",
        "print(\"Rows in Comments: \" + str(rowsComments))\n",
        "\n",
        "rowsBadges = badges.count()\n",
        "print(\"Rows in Badges:\" + str(rowsBadges))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rows in Posts:56218\n",
            "Rows in Users:91617\n",
            "Rows in Comments: 58736\n",
            "Rows in Badges:105641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRqL5KJeygDL"
      },
      "source": [
        "# **Task 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc43kykf1otD"
      },
      "source": [
        "###Task 2.1\n",
        "**Find the average length of the questions, answers, and comments in character.**\n",
        "\n",
        "Firstly  we  made  a  method  for  decoding.   Then  we  extracted  the  ”Body”  of  questions,  decodedeach body, found length and the average by stats().mean().  Did the same for answers and comments.  The main difference between the three was in the type of decoding.  Questions and answersused ”utf-8” decoding, while comments used ”ISO-8859-1” decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ItIIsbl1sOZ",
        "outputId": "ba1ed550-4e90-4ae8-897e-ce241057069b"
      },
      "source": [
        "import base64\n",
        "import re\n",
        "#Average length of questions in characters\n",
        "\n",
        "def decode(cleanText, encoding):\n",
        "  decodedText = cleanText.map(lambda x: str(base64.b64decode(x), encoding)) #decode from base64\n",
        "  cleanDecoded = re.compile('<.*?>')  #remove html tags\n",
        "  cleanText = decodedText.map(lambda x: re.sub(cleanDecoded, '', x).replace(\"&#xA;\",''))\n",
        "  return cleanText\n",
        "\n",
        "#Average length of answers in characters \n",
        "\n",
        "rddP.map(lambda x: x[postscolumns.index(\"PosttypeId\")])\n",
        "\n",
        "codedQuestions = rddP.filter(lambda x: x[postscolumns.index(\"PostTypeId\")] == \"1\").map(lambda x: x[postscolumns.index(\"Body\")])\n",
        "decodedQuestions = decode(codedQuestions, \"utf-8\")\n",
        "avLengthQuestions = decodedQuestions.map(lambda x: len(x)).stats().mean()\n",
        "print(str(avLengthQuestions))\n",
        "\n",
        "codedAnswers = rddP.filter(lambda x: x[postscolumns.index(\"PostTypeId\")] == \"2\").map(lambda x: x[postscolumns.index(\"Body\")])\n",
        "decodedAnswers = decode(codedAnswers, \"utf-8\")\n",
        "avLengthAnswers = decodedAnswers.map(lambda x: len(x)).stats().mean()\n",
        "print(str(avLengthAnswers))\n",
        "\n",
        "codedComments = rddC.map(lambda x: x[commentscolumns.index(\"Text\")])\n",
        "decodedComments = decode(codedComments, \"ISO-8859-1\")\n",
        "avLengthComments = decodedComments.map(lambda x: len(x)).stats().mean()\n",
        "print(str(avLengthComments))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "894.1216180219943\n",
            "794.282617409307\n",
            "168.8353309724881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_894IutU1vFK"
      },
      "source": [
        "###Task 2.2\n",
        "**Find the dates when the first and the last questions were asked. Also, find the display\n",
        "name of users who posted those questions **\n",
        "\n",
        "First we filtered on questions and extracted ”CreationDate” from the posts-rdd.  Performed reduce(min)  to  find  the  earliest  date.   Found  the  ”DisplayName”  by  indexing  with  corresponding”Id” in the users-rdd.  Found the last question posted by the same procedure, only replacing minby max in the reduce() operation. The first question was asked at **2014-05-13 23:58:30** and posted by **Doorknob**, and the last question was asked at **2020-12-06 03:01:58** and posted by **mon**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTp-3yZn1yPu",
        "outputId": "3de063cf-00db-4588-90af-204b197088a5"
      },
      "source": [
        "#Find all questions asked\n",
        "datesAndIds = rddP.filter(lambda x: x[postscolumns.index(\"PostTypeId\")] == \"1\").map(lambda x: (x[postscolumns.index(\"CreationDate\")], x[postscolumns.index(\"OwnerUserId\")]))\n",
        "\n",
        "#First question\n",
        "firstQuestionAsked = datesAndIds.reduce(min) #Will select by first index argumment, i.e. date\n",
        "print(firstQuestionAsked[0])\n",
        "firstName = rddU.filter(lambda x: x[userscolumns.index(\"Id\")] == firstQuestionAsked[1] ).map(lambda x: x[userscolumns.index(\"DisplayName\")]).collect()\n",
        "print(str(firstName[0]))\n",
        "\n",
        "#Last question\n",
        "lastQuestionAsked = datesAndIds.reduce(max) #Will select by first index argumment, i.e. date\n",
        "print(lastQuestionAsked[0])\n",
        "lastName = rddU.filter(lambda x: x[userscolumns.index(\"Id\")] == lastQuestionAsked[1] ).map(lambda x: x[userscolumns.index(\"DisplayName\")]).collect()\n",
        "print(str(lastName[0]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2014-05-13 23:58:30\n",
            "Doorknob\n",
            "2020-12-06 03:01:58\n",
            "mon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPgoHUXz11aJ"
      },
      "source": [
        "###Task 2.3\n",
        "**Find the ids of users who wrote the greatest number of answers and questions. Ignore the user with OwnerUserId equal to -1.**\n",
        "\n",
        "We started by removing posts from the posts-rdd which contained ”OwnerUserId” equal to ”NULL”(equivalent to -1). Then filtered on questions, extracted ”OwnerUserId” and performed reduceByKey to count the occurrence of each user.  Sorted by occurrences, collected into list and extracted the last elementto obtain the user with the greatest count of questions.  Same procedure for answers.The  UserId  of  of  the  user  that  has  written  the  greatest  number  of  questions  is **8820**,  and  the amount is **103 questions**. The  UserId  of  of  the  user  that  has  written  the  greatest  number  of  answers  is **64377**,  and  the amount is **579 answers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVHmGgFe14LV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d41ba7-e1c9-4325-a6fb-c7802b25371d"
      },
      "source": [
        "#Filter out users with ownerUserId == \"NULL\" (-1)\n",
        "ownerUserId = rddP.filter(lambda x: not(x[postscolumns.index(\"OwnerUserId\")] == \"NULL\"))\n",
        "\n",
        "#Find user with greatest number of questions\n",
        "userQuestions = ownerUserId.filter(lambda x: x[postscolumns.index(\"PostTypeId\")] == \"1\").map(lambda x: (x[postscolumns.index(\"OwnerUserId\")], 1))\n",
        "userQuestions = userQuestions.reduceByKey(lambda a,b: a+b)\n",
        "userQuestions = userQuestions.sortBy(lambda row: row[1]).collect()\n",
        "print(str(userQuestions[-1]))\n",
        "\n",
        "#Find user with greatest number of answers, same procedure as questions\n",
        "userAnswers = ownerUserId.filter(lambda x: x[postscolumns.index(\"PostTypeId\")] == \"2\").map(lambda x: (x[postscolumns.index(\"OwnerUserId\")], 1))\n",
        "userAnswers = userAnswers.reduceByKey(lambda a,b: a+b)\n",
        "userAnswers = userAnswers.sortBy(lambda row: row[1]).collect()\n",
        "print(str(userAnswers[-1]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('8820', 103)\n",
            "('64377', 579)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zg7jXFH16-6"
      },
      "source": [
        "###Task 2.4\n",
        "**Calculate the number of users who received less than three badges.**\n",
        "\n",
        "Started  by  extracting  ”UserId”  column  from  the  badges-rdd. Added  a  columns  of  ”1”  on  each row and performed reduceByKey to remove duplicates and count the occurrence of each unique id.  We then filtered to keep only the users with less than three badges and finally performed acount() operation to obtain the answer. The number of users with less than three badges is **37190**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7lIJz5b1_f0",
        "outputId": "035a5863-33d9-42f3-eeff-d0cb8e30d194"
      },
      "source": [
        "userBadge = rddB.map(lambda x: (x[badgescolumns.index(\"UserId\")], 1))\n",
        "userBadge = userBadge.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "amountLessThanThree = userBadge.filter(lambda x: (x[1]<3)).count()\n",
        "\n",
        "print(str(amountLessThanThree))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zbEmFmR2Bx4"
      },
      "source": [
        "###Task 2.5\n",
        "**Calculate the Pearson correlation coefficient (or Pearson’s r) between the number of\n",
        "upvotes and downvotes cast by a user. **\n",
        "\n",
        "We made a method implementing the logic of the formula with the use of a simple for-loop and numpy-lists. Found average of upvotes and downvotes by performing stats().mean() on respectively columns in the users-rdd. Collected upvotes and downvotes in each own list.  Fed this four arguments in the method for Pearsons coefficient and calculated the answer. \n",
        "\n",
        "The Pearson correlation coefficient calculated was **0.2684978771516632**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BowkfgEm2EjY",
        "outputId": "204ff50b-5ad3-41fd-fe0e-781dcc9c917f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def pearson(x,y, mean_x, mean_y):\n",
        "\n",
        "  teller = 0\n",
        "  nevnerx = 0\n",
        "  nevnery = 0\n",
        "\n",
        "  for i in range (0,len(x)):\n",
        "    teller += (x[i]-  mean_x)*(y[i]-mean_y)\n",
        "    nevnerx += (x[i]-mean_x)**2\n",
        "    nevnery += (y[i]-mean_y)**2\n",
        "\n",
        "  pearson = teller/(np.sqrt(nevnerx)*np.sqrt(nevnery))\n",
        "  return pearson\n",
        "\n",
        "ups = rddU.map(lambda x: x[userscolumns.index(\"UpVotes\")]).collect()[1:]\n",
        "ups = sc.parallelize(ups)\n",
        "avarageUp = ups.map(lambda x: float(x)).stats().mean()\n",
        "ups = ups.map(lambda x: float(x)).collect()\n",
        "\n",
        "downs = rddU.map(lambda x: x[userscolumns.index(\"DownVotes\")]).collect()[1:]\n",
        "downs = sc.parallelize(downs)\n",
        "avarageDown = downs.map(lambda x: float(x)).stats().mean()\n",
        "downs = downs.map(lambda x: float(x)).collect()\n",
        "\n",
        "print(pearson(ups, downs, avarageUp, avarageDown))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2684978771516632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtc3jqxL2GyM"
      },
      "source": [
        "###Task 2.6\n",
        "**Calculate the entropy of id of users (that is UserId column from comments data) who wrote one or more comments.**\n",
        "\n",
        "We found the total number of users by performing a count() operation on ”UserId” in the comments-rdd. Collected all UserdIds in a numpy list.  Could then implement the logic of the formula presented in the task by a simple ”for-loop” over the list of UserIds. The entropy calculated was **47.080874619623344**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYPtpjxA2JoI",
        "outputId": "fe70a5d3-35e7-4dcb-a8b4-a7d97bc3bdad"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def P(x, totalNumberOfUsers):\n",
        "  return x/totalNumberOfUsers\n",
        "\n",
        "def H():\n",
        "  userIds = rddC.map(lambda x: (x[commentscolumns.index(\"UserId\")],1))\n",
        "  userIds = userIds.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "  totalNumberOfUsers = userIds.count()\n",
        "  userIds = userIds.collect()\n",
        "\n",
        "  entropy = 0\n",
        "\n",
        "  for i in range (0,totalNumberOfUsers):\n",
        "    entropy -= P(userIds[i][1], totalNumberOfUsers)*np.log2(P(userIds[i][1], totalNumberOfUsers))\n",
        "\n",
        "  return entropy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  print(H())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47.080874619623344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUynvYSm2P3O"
      },
      "source": [
        "#Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQofrQZc2TYs"
      },
      "source": [
        "###Task 3.1\n",
        "Create a graph of posts and comments. Nodes are users, and there is an edge from node 𝑖 to node 𝑗 if 𝑖 wrote a comment for 𝑗’s post. Each edge has a weight 𝑤𝑖𝑗 that is the number of times 𝑖 has commented a post by 𝑗.\n",
        "\n",
        "The  idea was to make a rdd to represent the graph. We made a rdd with three columns, one with source (id of the user who made the comment), destination (id of the user who had madethe post) and a weight. This was achieved by making a new rdd of the posts-rdd, just containing the  id of the post and the ”OwnerUserId”.   Then we made another rdd of the comments-rdd, just containing the ”PostId” and the ”UserId”. Then we performed a join operation on the two new rdds, resulting in a new rdd. To be able to perform a reduceByKey operation to add up the weights for each unique, we were forced to have only two columns in our rdd.  To achieve this we put ”UserId” as source and ”OwnerUserId” as destination together as a list of length two within the key column.  Now we were able to perform the reduceByKey operation to add up the weightsin  the  graph.  To  make  the  rdd  complete  we  extracted  source  and  destination  to  each  separate column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3alzSvjibqVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee4fc415-b598-40ce-a686-4affca4cb076"
      },
      "source": [
        "from pyspark.sql import DataFrame\n",
        "\n",
        "userIds = rddP.map(lambda x: ( x[postscolumns.index(\"Id\")],x[postscolumns.index(\"OwnerUserId\")]))\n",
        "comments = rddC.map(lambda x: ((x[commentscolumns.index(\"PostId\")], x[commentscolumns.index(\"UserId\")])))\n",
        "\n",
        "result = userIds.join(comments).map(lambda x: (x[1],1))\n",
        "rddEdges = result.reduceByKey(lambda a,b: a+b)\n",
        "#Making it on the format src (=id of commenter), dst (=id of poster), w\n",
        "rddEdges = rddEdges.map(lambda x: (x[0][1], x[0][0], x[1]))\n",
        "\n",
        "print(str(rddEdges.collect()[:11]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('24', '22', 1), ('53', '66', 1), ('115', '84', 1), ('2723', '84', 1), ('21825', '84', 1), ('70', '96', 1), ('14', '14', 11), ('18481', '14', 1), ('434', '59', 1), ('13023', '59', 1), ('471', '151', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZByunsvWaMg2"
      },
      "source": [
        "###Task 3.2\n",
        "**Convert the result of the previous step into a Spark DataFrame (DF) and answer the following subtasks using DataFrame API, namely using Spark SQL.**\n",
        "\n",
        "This  was  accomplished  by  the  use  of  rdd’s  inbuilt  function  toDF().   We  could  perform  toDF()directly on the rdd we got from previous subtask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6iMKovVaS-k",
        "outputId": "2afa5672-6ee0-4b91-ebfc-1d64639a6866"
      },
      "source": [
        "dfEdges = rddEdges.toDF()\n",
        "dfEdges = dfEdges.withColumnRenamed('_1', \"src\").withColumnRenamed('_2', \"dst\").withColumnRenamed('_3', \"w\")\n",
        "\n",
        "dfEdges.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+---+\n",
            "|  src|dst|  w|\n",
            "+-----+---+---+\n",
            "|   24| 22|  1|\n",
            "|   53| 66|  1|\n",
            "|  115| 84|  1|\n",
            "| 2723| 84|  1|\n",
            "|21825| 84|  1|\n",
            "|   70| 96|  1|\n",
            "|   14| 14| 11|\n",
            "|18481| 14|  1|\n",
            "|  434| 59|  1|\n",
            "|13023| 59|  1|\n",
            "|  471|151|  1|\n",
            "|  146| 84|  3|\n",
            "|   84| 84| 10|\n",
            "| 1156| 84|  1|\n",
            "|  157|158|  1|\n",
            "|  158|158| 10|\n",
            "|  178|178|  4|\n",
            "|  249| 26|  1|\n",
            "|  189| 84|  1|\n",
            "|  116| 21|  3|\n",
            "+-----+---+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCPjOCjacrsG"
      },
      "source": [
        "###Task 3.3\n",
        "**Find the user ids of top 10 users who wrote the most comments**\n",
        "\n",
        "We  extracted  the  source-column  (”src”)  and  the  weight-column  (”w”)  from  the  DF  in  the  previous task.  Then we performed a sum() operation with respect to weigth, adding up all weights corresponding to the same source.  To find the top 10 users, we sorted the new DF by descending order and extracted the first 10 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohu7tUW-cxlR",
        "outputId": "4633f544-9fc0-4caa-e01b-5565fd887221"
      },
      "source": [
        "comment = dfEdges.groupBy(\"src\").sum().withColumnRenamed('sum(w)', \"w\")\n",
        "comment.sort(comment.w.desc()).select(\"src\").show(n=10)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|  src|\n",
            "+-----+\n",
            "|  836|\n",
            "|  381|\n",
            "|28175|\n",
            "|64377|\n",
            "|35644|\n",
            "|55122|\n",
            "|  924|\n",
            "|71442|\n",
            "|   21|\n",
            "|45264|\n",
            "+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAbA8OaQwDtq"
      },
      "source": [
        "###Task 3.4\n",
        "**Find the display names of top 10 users who their posts received the greatest number of comments. To do so, you can load users information (or table) into a DF and join the DF from previous subtasks (that the DF containing the graph of posts and comments) with it to produce the results.**\n",
        "\n",
        "We extracted the destination-column (”dst”) and the weight-column (”w”) from the DF in subtask3.2.  Then we performed a sum() operation with respect to weigth, adding up all weights corresponding to the same destination.  hen we made a new rdd of the users-rdd, containing ”Id” and ”DisplayName”, and removed the first row containing headers.  Converting the rdd to DF using toDF(), performed a left join, sorted the new DF by descending order, selecting ”DisplayName” and showing the first 10 rows, this gave us the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu46AjAB0hd8",
        "outputId": "ce3c7a07-c9f8-48de-8245-da3727c1c7cf"
      },
      "source": [
        "posts = dfEdges.groupBy(\"dst\").sum().withColumnRenamed('sum(w)', \"w\")\n",
        "\n",
        "users = rddU.map(lambda x: (x[userscolumns.index(\"Id\")], x[userscolumns.index(\"DisplayName\")])).collect()[1:]\n",
        "\n",
        "dfUsers = sc.parallelize(users).toDF()\n",
        "dfUsers = dfUsers.withColumnRenamed('_1', \"Id\").withColumnRenamed('_2', \"DisplayName\")\n",
        "\n",
        "dfMerge = dfUsers.join(posts, dfUsers.Id == posts.dst, how = 'left')\n",
        "dfMerge.sort(dfMerge.w.desc()).select(\"DisplayName\").show(10)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|         DisplayName|\n",
            "+--------------------+\n",
            "|         Neil Slater|\n",
            "|               Erwan|\n",
            "|               Media|\n",
            "|             n1k31t4|\n",
            "|Has QUIT--Anony-M...|\n",
            "|            JahKnows|\n",
            "|               Leevo|\n",
            "|         David Masip|\n",
            "|          Noah Weber|\n",
            "|      Brian Spiering|\n",
            "+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYTb1qAGkWLj"
      },
      "source": [
        "###Task 3.5\n",
        "\n",
        "**Save the DF containing the information for the graph of posts and comments (from subtask 2) into a persistence format (like CSV) on your filesystem so that later could be loaded back into a Spark application’s workspace**\n",
        "\n",
        "This subtask was accomplished by using DF’s inbuilt function write.csv.  Path to the csv’s:  ”/con-tent/edges.csv”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wICOE4oAkbsL"
      },
      "source": [
        "dfEdges.write.csv('edges.csv')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJNfXJV04bDW"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Throughout this project,  we have learnt a great deal about Big Data and how to process somedata with Spark.  It has become clearer to us how Big Data is a powerful tool to reveal interestinginsights and information.  To execute different queries on the DFs was relatively straight forward,while we encountered some difficulties looking into the RDDs.  In task 3, we were not sure abouthow to go through with GraphFrame, as the task was worded quite ambigious.  We, and assuming others have had trouble with the same, it would be useful to have some sort of documentation onGraphFrame, especially in Python, as most found online was written in Scala.  In conclusion, we believe we have obtained the results we wanted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGSL24PK4yS1"
      },
      "source": [
        "## Bibliography"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwhjOqrm42UT"
      },
      "source": [
        "[1]Spark - PPT TDT4305 2019\n",
        "\n",
        "[2] Intro til TDT4305 Big Data-arkitektur Våren 2021\n",
        "\n",
        "[3]    tutorialspoint.com : Apache  Spark  RDD\n",
        "url:https://www.tutorialspoint.com/apachespark/apachesparkrdd.htm (visited on 02/03/2020)"
      ]
    }
  ]
}