{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prosjekt del 2 - Emil Neby + Cornelia Plesner",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilnebb/TDT4305_Big_Data_Architecture/blob/main/Prosjekt_del_2_Emil_Neby_%2B_Cornelia_Plesner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB16uBsJDrxY"
      },
      "source": [
        "### I used contents from these sources to create this Colab notebook: \n",
        "  1. https://colab.research.google.com/github/asifahmed90/pyspark-ML-in-Colab/blob/master/PySpark_Regression_Analysis.ipynb\n",
        "  2. https://gist.github.com/dvainrub/b6178dc0e976e56abe9caa9b72f73d4a\n",
        "  3. https://towardsdatascience.com/graphframes-in-jupyter-a-practical-guide-9b3b346cebc5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LupFC2QBLB9P"
      },
      "source": [
        "# **OUTCOME: having an enviornment to develop Spark apps in Python3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "## **Step 0: setting things up in Google Colab**\n",
        "\n",
        "First, we need to install all the dependencies in Colab environment like Apache `Spark 3 with Hadoop 2.7`, `Python3`, `Java 11` (and a helper Python package named `Findspark`). \n",
        "\n",
        "Please note that you might need to update Spark's version to a newer value if, after executing the code in the cell bellow, you get an error like `wget` can't find and download `spark-3.0.2-*`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "759b912c-2b04-41c7-855c-435443de7393"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://bitbucket.org/habedi/datasets/raw/b6769c4664e7ff68b001e2f43bc517888cbe3642/spark/spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!rm -rf spark-3.0.2-bin-hadoop2.7.tgz*\n",
        "!pip -q install findspark pyspark graphframes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.3MB 81kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 20.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 57.5MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set some environment variables. We need to set the values for `JAVA_HOME` and `SPARK_HOME` (and `HADOOP_HOME`), as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\"\n",
        "os.environ[\"HADOOP_HOME\"] = os.environ[\"SPARK_HOME\"]\n",
        "\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mjULZnTHVnU"
      },
      "source": [
        "## **Step 1: downloading project's dataset**\n",
        "Now let's download the project's dataset from Github. You can read the dataset for the course's project from `datasets/data/TDT4305_S2021`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG6eGOe7Hcu_",
        "outputId": "347fdfa8-f2d0-4e38-f568-19e184832c4e"
      },
      "source": [
        "!rm -rf datasets\n",
        "!git clone --depth=1 -q https://github.com/habedi/datasets\n",
        "!ls datasets/data/TDT4305_S2021"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " badges.csv.gz\t  'Description of the data.pdf'   users.csv.gz\n",
            " comments.csv.gz   posts.csv.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "## **Step 2: checking the Spark installation**\n",
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48lL1JV3M87K"
      },
      "source": [
        "## **Step 3: making a helper method for creating a SaprkContext variable**\n",
        "You can use `init_spark` to create a new `SaprkContext variable` and use it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKP2o0UyIvFZ"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def init_spark(app_name=\"HelloWorldApp\", execution_mode=\"local[*]\"):\n",
        "  spark = SparkSession.builder.master(execution_mode).appName(app_name).getOrCreate()\n",
        "  sc = spark.sparkContext\n",
        "  return spark, sc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2HGl1xmN7pI"
      },
      "source": [
        "## **Step 4: a HelloWorld Spark app**\n",
        "\n",
        "Our first Spark application; it takes a list of numbers and squares each element and returns the list of squared numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnR5C9jhBHsB",
        "outputId": "a94da478-b9c4-41c0-c018-4a8a309a1185"
      },
      "source": [
        "def main1():\n",
        "  _, sc = init_spark()\n",
        "  nums = sc.parallelize([1, 2, 3, 4])\n",
        "  print(nums.map(lambda x: x*x).collect())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main1()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 4, 9, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHA_COdYOKnO"
      },
      "source": [
        "## **Step 5: another Saprk app that loades a CSV files into an RDD**\n",
        "Another simple app that prints the first two lines of from `users.csv.gz`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlzNiahyCH9w",
        "outputId": "a8fa050d-d396-478c-c96a-bd3662e7e1d5"
      },
      "source": [
        "def main2():\n",
        "  _, sc = init_spark()\n",
        "  lines = sc.textFile('datasets/data/TDT4305_S2021/users.csv.gz')\n",
        "  print(lines.take(2))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main2()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\"Id\"\\t\"Reputation\"\\t\"CreationDate\"\\t\"DisplayName\"\\t\"LastAccessDate\"\\t\"AboutMe\"\\t\"Views\"\\t\"UpVotes\"\\t\"DownVotes\"', \"-1\\t1\\t2014-05-13 21:29:22\\tCommunity\\t2014-05-13 21:29:22\\t<p>Hi, I'm not really a person.</p>&#xA;&#xA;<p>I'm a background process that helps keep this site clean!</p>&#xA;&#xA;<p>I do things like</p>&#xA;&#xA;<ul>&#xA;<li>Randomly poke old unanswered questions every hour so they get some attention</li>&#xA;<li>Own community questions and answers so nobody\\t3\\t819\\t1575\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAEwctyiTkju"
      },
      "source": [
        "## **Step 6: sample GraphFrames code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIRkcwMHTvyH",
        "outputId": "06e00538-da03-4006-d7af-715811406612"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "_, sc = init_spark()\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "## the rest of this code (down below) comes from: https://graphframes.github.io/graphframes/docs/_site/quick-start.html#getting-started-with-apache-spark-and-spark-packages\n",
        "\n",
        "# Create a Vertex DataFrame with unique ID column \"id\"\n",
        "v = sqlContext.createDataFrame([\n",
        "  (\"a\", \"Alice\", 34),\n",
        "  (\"b\", \"Bob\", 36),\n",
        "  (\"c\", \"Charlie\", 30),\n",
        "], [\"id\", \"name\", \"age\"])\n",
        "\n",
        "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
        "e = sqlContext.createDataFrame([\n",
        "  (\"a\", \"b\", \"friend\"),\n",
        "  (\"b\", \"c\", \"follow\"),\n",
        "  (\"c\", \"b\", \"follow\"),\n",
        "], [\"src\", \"dst\", \"relationship\"])\n",
        "\n",
        "# Create a GraphFrame\n",
        "from graphframes import *\n",
        "g = GraphFrame(v, e)\n",
        "\n",
        "# Query: Get in-degree of each vertex.\n",
        "g.inDegrees.show()\n",
        "\n",
        "# Query: Count the number of \"follow\" connections in the graph.\n",
        "g.edges.filter(\"relationship = 'follow'\").count()\n",
        "\n",
        "# Run PageRank algorithm, and show results.\n",
        "#results = g.pageRank(resetProbability=0.01, maxIter=10)\n",
        "#results.vertices.select(\"id\", \"pagerank\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+\n",
            "| id|inDegree|\n",
            "+---+--------+\n",
            "|  c|       1|\n",
            "|  b|       2|\n",
            "+---+--------+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgf8F1ElW6sz"
      },
      "source": [
        "### See: https://towardsdatascience.com/graphframes-in-jupyter-a-practical-guide-9b3b346cebc5 for more examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3jnIXx0OOI2"
      },
      "source": [
        "\n",
        "## **Step 7 and beyond: create your apps down here (as many as you need)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvf6Pw4oRxg1"
      },
      "source": [
        "##Constructing the graph of terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1BQDFJSJxG6"
      },
      "source": [
        "#Clone list of stopwords\n",
        "!git clone --depth=1 -q https://gist.github.com/habedi/c7229ee5bd50bf49f5b2bc404366344d\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UMYks7wmxOY"
      },
      "source": [
        "###Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jubenTqLZim"
      },
      "source": [
        "import string\n",
        "\n",
        "#return a list with stopwords\n",
        "def get_stopwords():\n",
        "  stopwords = []\n",
        "  f = open('c7229ee5bd50bf49f5b2bc404366344d/big_list_of_english_stopwords', \"r\")\n",
        "\n",
        "  for line in f:\n",
        "    stopwords.append(line[:-1])\n",
        "\n",
        "  return stopwords\n",
        "\n",
        "#Remove all special characters, except 'DOT', and remove 'TAB'\n",
        "def remove_special_characters(document):\n",
        "  for i in document:\n",
        "    if (i in string.punctuation) and not(i == \".\"):\n",
        "      document = document.replace(i, ' ')\n",
        "\n",
        "  document = document.replace('\\n', ' ')\n",
        "  return document\n",
        "\n",
        "#Remove words with fewer than three characters\n",
        "def remove_smaller_3(words):\n",
        "  for word in words[:]:\n",
        "    if len(word)< 3:\n",
        "      words.remove(word)\n",
        "\n",
        "  return words\n",
        "\n",
        "#Remove 'DOT' at start/end of token\n",
        "def remove_DOT(words):\n",
        "  for i in range(0, len(words)):\n",
        "    if (words[i][0] == '.') or (words[i][-1] == '.'):\n",
        "      words[i] = words[i].replace('.', '')\n",
        "\n",
        "  return words\n",
        "\n",
        "#Remove stopwords\n",
        "def remove_stopwords(words):\n",
        "  stopwords = get_stopwords()\n",
        "  new_words = []\n",
        "\n",
        "  for word in words:\n",
        "    if word not in stopwords:\n",
        "      new_words.append(word)\n",
        "\n",
        "  return new_words\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5PDZYDa1T6K"
      },
      "source": [
        "###Constructing a sequence of terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC4mqudLpLmT"
      },
      "source": [
        "#Output: sequence of terms, i.e. a list of strings\n",
        "#Input arg: string\n",
        "\n",
        "def sequence_of_terms(document):\n",
        "  #1. Turn all characters to lower case\n",
        "  lower = document.lower()\n",
        "\n",
        "  #2. Remove all the punctuations (like '!' and '?') except 'DOT' characters\n",
        "  #3. Remove all the symbols (like '$' and '>') and special characters (like 'TAB')\n",
        "  lower = remove_special_characters(lower)\n",
        "\n",
        "  #4. Tokenise the output of the previous step (the separator of tokens is the 'WHITESPACE' character); at this stage should have a sequence of tokens\n",
        "  tokens = lower.split(\" \")\n",
        "\n",
        "  #5. Remove the tokens that are smaller than three characters long from the sequence of the tokens\n",
        "  sequence = remove_smaller_3(tokens)\n",
        "\n",
        "  #6. Remove all the 'DOT' characters from the start or the end of each token\n",
        "  sequence = remove_DOT(sequence)\n",
        "\n",
        "  #7. Remove the stopwords from the sequence of tokens\n",
        "  sequence = remove_stopwords(sequence)\n",
        "\n",
        "  return sequence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP4kWuisw7fD"
      },
      "source": [
        "###Constructing the graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJP1YELywlg8"
      },
      "source": [
        "import itertools\n",
        "from graphframes import *\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "#Output: rdd containing edges\n",
        "#Input arg: sequence of terms\n",
        "def edges(terms):\n",
        "  window = []\n",
        "  edges = []\n",
        "  terms_que = terms \n",
        "\n",
        "  while len(terms_que) > 1:\n",
        "    if (len(window) < 5):\n",
        "      window.append(terms_que[0])\n",
        "      terms_que.remove(terms_que[0])\n",
        "    else:\n",
        "      window.remove(window[0])\n",
        "      window.append(terms_que[0])\n",
        "      terms_que.remove(terms_que[0])\n",
        "\n",
        "    if (len(window) == 5):\n",
        "      tuples = list(itertools.permutations(window, 2))\n",
        "\n",
        "      for tup in tuples:\n",
        "        if not (tup[0] == tup[1]):\n",
        "          edges.append(tup)\n",
        " \n",
        "  rddE = sc.parallelize(edges)\n",
        "  rddE = rddE.distinct().map(lambda x: (x[0], x[1], \"neighboors\"))\n",
        "  return rddE\n",
        "\n",
        "#Output: rdd containing vertices\n",
        "#Input arg: sequence of terms \n",
        "def vertices(terms):\n",
        "  rddV = sc.parallelize(terms)\n",
        "  rddV = rddV.map(lambda x: (x, 1))\n",
        "  rddV = rddV.reduceByKey(lambda a,b: a+b).zipWithIndex().map(lambda x: ( x[0][0], x[1]))\n",
        "  return rddV\n",
        "\n",
        "#Output: graphFrame\n",
        "#Input arg: vertices as rdd, edges as rdd\n",
        "def graph(vertices, edges):\n",
        "  print(\"Graph is under construction... \\n\")\n",
        "  #Create dataframes of vertices and edges\n",
        "  edges = sqlContext.createDataFrame( edges.collect(), [\"src\", \"dst\", \"relationship\"])\n",
        "  vertices = sqlContext.createDataFrame( vertices.collect(), [\"id\", \"name\"])\n",
        "\n",
        "  g = GraphFrame(vertices, edges)\n",
        "  print(\"Vertices: \\n\")\n",
        "  g.vertices.show()\n",
        "  print(\"Edges: \\n\")\n",
        "  g.edges.show()\n",
        "\n",
        "  return g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKcG2gCGGkj5"
      },
      "source": [
        "###Methods to get body of post with given path and id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI8XBZE72WrC"
      },
      "source": [
        "import base64\n",
        "import re\n",
        "\n",
        "#Output: rdd containing posts\n",
        "#Input arg: directory path\n",
        "def loadPosts(path):\n",
        "  _, sc = init_spark()\n",
        "  rddP = sc.textFile(path)\n",
        "  rddP = rddP.map(lambda x: x.split('\\t'))\n",
        "  print(\"The 'posts.csv.gz' file is loaded into the RDD 'rddP'\")\n",
        "  return rddP\n",
        "\n",
        "#Output: body as a string\n",
        "#Input arg: directory path, id\n",
        "def getPostBody(path, id):\n",
        "  rddP = loadPosts(path)\n",
        "  postscolumns = ['Id', 'PostTypeId', 'CreationDate','Score','ViewCount',\"Body\",'OwnerUserId','LastActivityDate',\"Title\",\"Tags\",'AnswerCount','CommentCount','FavoriteCount','Closedate']\n",
        "\n",
        "  codedBody = rddP.filter(lambda x: x[postscolumns.index('Id')] == id).map(lambda x: x[postscolumns.index(\"Body\")])\n",
        "  codedBody = str(codedBody.collect()[0])\n",
        "  body = str(base64.b64decode(codedBody), \"utf-8\")\n",
        "  return body\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1GxXI4dxcg7"
      },
      "source": [
        "###Pagerank program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE6kMlGhAWem",
        "outputId": "17497e37-4044-474c-eea1-27a29485ab38"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "import base64\n",
        "import re\n",
        "\n",
        "#Input arg: directory path, id\n",
        "def pagerank_Program(path, id):\n",
        "  document = getPostBody(path, id)\n",
        "  doc1 = sequence_of_terms(document)\n",
        "  doc2 = sequence_of_terms(document)\n",
        "\n",
        "  edge = edges(doc1)\n",
        "  vertice = vertices(doc2)\n",
        "\n",
        "  graph_of_terms = graph(vertice, edge)\n",
        "\n",
        "  print(\"Calculating pagerank... \\n\")\n",
        "  pagerank = graph_of_terms.pageRank(resetProbability= 0.15, tol= 0.0001)\n",
        "\n",
        "  print(\"PageRank score for top 10 nodes: \\n\")\n",
        "  pagerank.vertices.select(\"id\",\"pagerank\").orderBy( desc(\"pagerank\")).show(10)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  directory_path = 'datasets/data/TDT4305_S2021/posts.csv.gz'\n",
        "  post_id = '14'\n",
        "  \n",
        "  pagerank_Program(directory_path, post_id)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 'posts.csv.gz' file is loaded into the RDD 'rddP'\n",
            "Graph is under construction... \n",
            "\n",
            "Vertices: \n",
            "\n",
            "+-----------+----+\n",
            "|         id|name|\n",
            "+-----------+----+\n",
            "|    science|   0|\n",
            "|     fields|   1|\n",
            "|      large|   2|\n",
            "|   question|   3|\n",
            "|     mining|   4|\n",
            "|   graduate|   5|\n",
            "|      class|   6|\n",
            "|      years|   7|\n",
            "|differences|   8|\n",
            "| proficient|   9|\n",
            "|       data|  10|\n",
            "|  discussed|  11|\n",
            "|      forum|  12|\n",
            "|   synonyms|  13|\n",
            "|   analyzed|  14|\n",
            "+-----------+----+\n",
            "\n",
            "Edges: \n",
            "\n",
            "+---------+---------+------------+\n",
            "|      src|      dst|relationship|\n",
            "+---------+---------+------------+\n",
            "|     data|discussed|  neighboors|\n",
            "|     data|    forum|  neighboors|\n",
            "|     data| synonyms|  neighboors|\n",
            "|discussed|     data|  neighboors|\n",
            "|discussed|    forum|  neighboors|\n",
            "|discussed| synonyms|  neighboors|\n",
            "|    forum|     data|  neighboors|\n",
            "|    forum|discussed|  neighboors|\n",
            "|    forum| synonyms|  neighboors|\n",
            "| synonyms|     data|  neighboors|\n",
            "| synonyms|discussed|  neighboors|\n",
            "| synonyms|    forum|  neighboors|\n",
            "|  science|   fields|  neighboors|\n",
            "|   fields|  science|  neighboors|\n",
            "|   fields|    large|  neighboors|\n",
            "|    large|   fields|  neighboors|\n",
            "| synonyms| analyzed|  neighboors|\n",
            "|     data| analyzed|  neighboors|\n",
            "| analyzed| synonyms|  neighboors|\n",
            "| analyzed|     data|  neighboors|\n",
            "+---------+---------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Calculating pagerank... \n",
            "\n",
            "PageRank score for top 10 nodes: \n",
            "\n",
            "+--------+------------------+\n",
            "|      id|          pagerank|\n",
            "+--------+------------------+\n",
            "|    data|1.8654218671868266|\n",
            "|  mining|1.2609991383306336|\n",
            "| science|1.2550464922135118|\n",
            "|  fields|1.1020312104525887|\n",
            "|question|0.9850216709761106|\n",
            "|   large|0.9814216116939003|\n",
            "|analyzed|0.9804672747252674|\n",
            "|synonyms|0.9804629823034164|\n",
            "|   class| 0.876518306223545|\n",
            "|   years|0.8755763969231413|\n",
            "+--------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}